{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223f7a35-2919-4e10-9222-31ee241120be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on AND gate:\n",
      "Learned Weights for AND: [0.05000000000000002, 9.71445146547012e-17]\n",
      "Epochs Taken for AND: 999\n",
      "\n",
      "Training on XOR gate:\n",
      "Learned Weights for XOR: [-0.04999999999999999, -0.049999999999999906]\n",
      "Epochs Taken for XOR: 999\n",
      "\n",
      "Weighted Sum for inputs [1, 2, 3] and weights [0.5, -1, 2]: 4.5\n",
      "Step Activation: 1\n",
      "Bipolar Step Activation: 1\n",
      "Sigmoid Activation: 0.9890130573694068\n",
      "TanH Activation: 0.9997532108480275\n",
      "ReLU Activation: 4.5\n",
      "Leaky ReLU Activation: 4.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# A1: Summation Unit\n",
    "def a1(inputs, weights):\n",
    "    \"\"\"\n",
    "    Calculate the weighted sum of inputs and weights.\n",
    "    \n",
    "    :param inputs: list of input values\n",
    "    :param weights: list of weight values\n",
    "    :return: weighted sum\n",
    "    \"\"\"\n",
    "    return sum(i * w for i, w in zip(inputs, weights))\n",
    "\n",
    "\n",
    "# A2: Activation Functions\n",
    "\n",
    "# Step Activation Function\n",
    "def a2_step_activation(value):\n",
    "    return 1 if value > 0 else 0\n",
    "\n",
    "# Bipolar Step Activation Function\n",
    "def a2_bipolar_step_activation(value):\n",
    "    return 1 if value > 0 else -1\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "def a2_sigmoid_activation(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "# TanH Activation Function\n",
    "def a2_tanh_activation(value):\n",
    "    return math.tanh(value)\n",
    "\n",
    "# ReLU Activation Function\n",
    "def a2_relu_activation(value):\n",
    "    return max(0, value)\n",
    "\n",
    "# Leaky ReLU Activation Function\n",
    "def a2_leaky_relu_activation(value, alpha=0.01):\n",
    "    return value if value > 0 else alpha * value\n",
    "\n",
    "\n",
    "# A3: Error Calculation\n",
    "def a3(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the error between actual and predicted values.\n",
    "    \n",
    "    :param actual: list of actual values\n",
    "    :param predicted: list of predicted values\n",
    "    :return: sum squared error\n",
    "    \"\"\"\n",
    "    return sum((a - p) ** 2 for a, p in zip(actual, predicted))\n",
    "\n",
    "\n",
    "# A4: Perceptron Learning Function\n",
    "def a4(inputs, outputs, weights, learning_rate, epochs=1000, convergence_error=0.002):\n",
    "    \"\"\"\n",
    "    Train a perceptron using step activation function for AND/XOR gate logic.\n",
    "    \n",
    "    :param inputs: list of input tuples\n",
    "    :param outputs: list of actual outputs\n",
    "    :param weights: list of initial weights\n",
    "    :param learning_rate: learning rate for weight adjustment\n",
    "    :param epochs: maximum number of iterations\n",
    "    :param convergence_error: error threshold for convergence\n",
    "    :return: learned weights, epoch count\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    errors = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for input_vec, expected_output in zip(inputs, outputs):\n",
    "            weighted_sum = a1(input_vec, weights)\n",
    "            predicted_output = a2_step_activation(weighted_sum)\n",
    "            error = expected_output - predicted_output\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Update weights\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] += learning_rate * error * input_vec[i]\n",
    "\n",
    "        errors.append(total_error)\n",
    "        if total_error <= convergence_error:\n",
    "            break\n",
    "    \n",
    "    return weights, epoch, errors\n",
    "\n",
    "\n",
    "# Example Usage for AND Gate Logic\n",
    "print(\"Training on AND gate:\")\n",
    "and_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "and_outputs = [0, 0, 0, 1]  # Expected outputs for AND gate\n",
    "initial_weights = [0.2, -0.75]  # Example initial weights\n",
    "learning_rate = 0.05  # Learning rate\n",
    "\n",
    "# Train the Perceptron on AND Gate\n",
    "learned_weights_and, epochs_taken_and, error_values_and = a4(and_inputs, and_outputs, initial_weights, learning_rate)\n",
    "\n",
    "# Display Results for AND Gate\n",
    "print(f\"Learned Weights for AND: {learned_weights_and}\")\n",
    "print(f\"Epochs Taken for AND: {epochs_taken_and}\")\n",
    "\n",
    "# Example Usage for XOR Gate Logic\n",
    "print(\"\\nTraining on XOR gate:\")\n",
    "xor_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "xor_outputs = [0, 1, 1, 0]  # Expected outputs for XOR gate\n",
    "initial_weights_xor = [0.2, -0.75]  # Example initial weights for XOR\n",
    "\n",
    "# Train the Perceptron on XOR Gate\n",
    "learned_weights_xor, epochs_taken_xor, error_values_xor = a4(xor_inputs, xor_outputs, initial_weights_xor, learning_rate)\n",
    "\n",
    "# Display Results for XOR Gate\n",
    "print(f\"Learned Weights for XOR: {learned_weights_xor}\")\n",
    "print(f\"Epochs Taken for XOR: {epochs_taken_xor}\")\n",
    "\n",
    "# Test the Summation Unit with Example Inputs and Weights\n",
    "example_inputs = [1, 2, 3]\n",
    "example_weights = [0.5, -1, 2]\n",
    "weighted_sum_result = a1(example_inputs, example_weights)\n",
    "\n",
    "print(f\"\\nWeighted Sum for inputs {example_inputs} and weights {example_weights}: {weighted_sum_result}\")\n",
    "\n",
    "# Example with Different Activation Functions\n",
    "print(f\"Step Activation: {a2_step_activation(weighted_sum_result)}\")\n",
    "print(f\"Bipolar Step Activation: {a2_bipolar_step_activation(weighted_sum_result)}\")\n",
    "print(f\"Sigmoid Activation: {a2_sigmoid_activation(weighted_sum_result)}\")\n",
    "print(f\"TanH Activation: {a2_tanh_activation(weighted_sum_result)}\")\n",
    "print(f\"ReLU Activation: {a2_relu_activation(weighted_sum_result)}\")\n",
    "print(f\"Leaky ReLU Activation: {a2_leaky_relu_activation(weighted_sum_result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d65fd1-5848-43c0-aaca-500b6d56fe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
